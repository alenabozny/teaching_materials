{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vf22xcU-7amZ"
      },
      "source": [
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<center>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "    <h1 style = \"font-size:24px; font-weight:normal\">SKRYPT DO LABORATORIUM</style></h1>\n",
        "    <h1 style = \"font-size:24px\">Przetwarzanie tekstu</style></h1>\n",
        "    <h1 style = \"font-size:24px; font-weight:normal\">LABORATORIUM 1:</style></h1>\n",
        "    <h1 style = \"font-size:24px\">Aleksandra Nabożny</style></h1>\n",
        "</center>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "<br>\n",
        "    </body>\n",
        "    </html>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Wprowadzenie**\n",
        "\n",
        "Znają już Państwo z wykładu podstawowe zagadnienia związane z modelem wektorowym służącym do obliczania podobieństwa pomiędzy tekstami (np. parą zapytanie-dokument). Powinni być już Państwo świadomi, że duży wpływ na skuteczność funkcji podobieństwa ma nie tylko jej sposób działania, ale także odpowiednie przygotowanie danych wejściowych. \n",
        "\n",
        "Niniejsze laboratorium poświęcone jest właśnie przygotowywaniu danych tekstowych. Poeksperymentują Państwo z najpopularniejszymi bibliotekami do przetwarzania tekstu."
      ],
      "metadata": {
        "id": "qfpHcA5eraW4"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFzYSedyq03d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c7d4a1b-30fe-47e5-c623-9c737546a5c7"
      },
      "source": [
        "import nltk\n",
        "import spacy\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwIIYsGOiKCF"
      },
      "source": [
        "# Tokenizacja i lematyzacja\n",
        "\n",
        "Pierwszym elementem pracy nad tekstem powinno być zawsze określenie, jakie informacje są nam potrzebne. Załóżmy, że zależy nam na pozyskaniu dokumentów dziedzinowych z dużej kolekcji dokumentów na różne tematy. Będziemy wówczas zwracali uwagę na *słownictwo* znajdujące się w tekście, ale już niekoniecznie na formę, w jakiej dane słowa zostały użyte. To może być, przykładowo, klasyfikacja na teksty dotyczące medycyny / niedotyczące medycyny. Słownictwo, które nas będzie interesowało, to głównie rzeczowniki i czasowniki pochodzące ze słownika medycznego.\n",
        "\n",
        "Przyda nam się zatem:\n",
        "- podział tekstu na słowa\n",
        "- algorytm pozwalający wydobyć samo słowo bazowe, bez odmiany (np. 'drink' zamiast 'drinking', 'drank', 'drinks').\n",
        "\n",
        "*! Proszę zwrócić uwagę na to, że lematyzacja nie jest zagadnieniem trywialnym! Słowa w różnych odmianach mogą być synonimami słów o innych znaczeniach (np. 'drinks' może być zarówno czasownikiem czasu present simple w 3 osobie, jak i rzeczownikiem w liczbie mnogiej). Zagadnienie związane z rozróżnianiem sensu słów w kontekście nazywa się Word Sense Disambiguation (WSD) i jest realizowane na różne sposoby, m.in. z wykorzystaniem modeli probabilistycznych. Wykorzystując gotowe narzędzia do WSD zawsze przeczytaj w dokumentacji, o jakie modele jest oparte!*\n",
        "\n",
        "Innym przykładem może być klasyfikacja tekstów literackich względem autorstwa. Słownictwo nie będzie nas już tak bardzo interesowało, jak np. cechy stylistyczne (średnia długość zdania, n-gramy części mowy, częstotliwość poszczególnych znaków interpunkcyjnych, etc.). W końcu każdy artysta ma swój unikatowy styl!\n",
        "\n",
        "Wróćmy się jednak do przykładu słownikowego i spróbujmy wykonać pierwsze zadania:\n",
        "- zapoznanie się z biblioteką Natural Language ToolKit (NLTK)\n",
        "- przeprowadzenie tokenizacji oraz lematyzacji na przykładowym zbiorze danych."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import reuters\n",
        "\n",
        "nltk.download('reuters')"
      ],
      "metadata": {
        "id": "Doyvjcnxr8gH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /root/nltk_data/corpora/reuters.zip -d /root/nltk_data/corpora/."
      ],
      "metadata": {
        "id": "1htlSLAG2IGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# funkcje pomocnicze do pracy z tekstami z agencji Reutera w NLTK\n",
        "\n",
        "def concatenate_reuters_paragraphs():\n",
        "  bodies = []\n",
        "  \n",
        "  for paragraph in reuters.paras():\n",
        "    body = ''\n",
        "    for sentence in paragraph:\n",
        "      s = ' '.join(sentence)\n",
        "      body = body + s\n",
        "    bodies.append(body)\n",
        "\n",
        "  return bodies\n",
        "\n",
        "def concatenate_reuters_full(reuters_paragraphs):\n",
        "  return ' '.join(reuters_paragraphs)"
      ],
      "metadata": {
        "id": "KAiVU0FS1c0g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reuters_paragraphs = concatenate_reuters_paragraphs() # teksty rozdzielone w liście\n",
        "reuters_full = concatenate_reuters_full(reuters_paragraphs) # scalone wszystkie teksty z korpusu"
      ],
      "metadata": {
        "id": "xMJs7-MT_rhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reuters_full"
      ],
      "metadata": {
        "id": "lEL2RhMY4iOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# funkcja pomocnicza do pracy z tekstami z agencji Reutera i pakietem BM25\n",
        "\n",
        "def tokenized_reuters(corpus = reuters.paras()):\n",
        "  tokenized_reuters = []\n",
        "\n",
        "  for news in corpus:\n",
        "    paragraph_body = []\n",
        "    for sentence in news:\n",
        "      paragraph_body = paragraph_body + sentence\n",
        "\n",
        "    tokenized_reuters.append(paragraph_body)\n",
        "  \n",
        "  return tokenized_reuters"
      ],
      "metadata": {
        "id": "s0r0ZzTxZvL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_reuters = tokenized_reuters()"
      ],
      "metadata": {
        "id": "WfIzyViFbDAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ah4ci0Siq2H_"
      },
      "source": [
        "## Przykład 1\n",
        "\n",
        "Ile słów znajduje się w tekście?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t11_winnrQWo"
      },
      "source": [
        "def word_count(text):\n",
        "    \n",
        "    return len(nltk.word_tokenize(text))\n",
        "\n",
        "#word_count(reuters_full)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeqXX76frg0w"
      },
      "source": [
        "## Przykład 2\n",
        "\n",
        "Jak wiele tokenów znajduje się w tekście (token to zarówno słowo, jak i znak interpunkcyjny)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "svw3aDI6rrSa"
      },
      "source": [
        "def token_count(collection):\n",
        "    \n",
        "    return len(set(nltk.word_tokenize(collection)))\n",
        "\n",
        "#token_count(reuters_full)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmEsYCSmr2XR"
      },
      "source": [
        "## Ćwiczenie 1 (1 pkt)\n",
        "\n",
        "Po lematyzacji, ile *unikatowych* tokenów znajduje się w tekście?\n",
        "\n",
        "! Jeżeli chcesz w łatwy sposób pozyskać zbiór (czyli listę bez powtórzeń) w Pythonie, wykorzystaj klasę **set**. \n",
        "\n",
        "Do tego zadania [wykorzytaj WordNetLemmatizer z biblioteki NLTK](https://www.nltk.org/api/nltk.stem.wordnet.html). \n",
        "\n",
        "W jaki sposób wykorzystany przez Ciebie moduł realizuje lematyzację? Dokop się do źródeł i odpowiedz na to pytanie poniżej. \n",
        "\n",
        "Model znajduje zlemmantyzowane słowo z bazy Wordnet, w przeciwnym przypadku (jeśli słowa nie ma w bazie) zwraca to samo słowo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROK2YUdvsVTP"
      },
      "source": [
        "from nltk.stem import ...\n",
        "import nltk\n",
        "\n",
        "def unique_lemmatized_token_count(collection):\n",
        "\n",
        "  ## wpisz treść swojej funkcji\n",
        "\n",
        "  return ...\n",
        "\n",
        "#print(reuters.paras()[0])\n",
        "unique_lemmatized_token_count(reuters_full)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjOWvfDWvbTc"
      },
      "source": [
        "## Ćwiczenie 2 (1 pkt)\n",
        "\n",
        "Jakia jest różnorodność leksykalna zbioru (tj. policz stosunek unikatowych tokenów, do wszystkich tokenów znajdujących się w korpusie)?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mE58mHkOvrW7"
      },
      "source": [
        "def lexical_diversity(coll):\n",
        "\n",
        "  ## wpisz treść swojej funkcji\n",
        "\n",
        "  return ...\n",
        "\n",
        "lexical_diversity(reuters_full)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmn6XMHwvz5R"
      },
      "source": [
        "## Ćwiczenie 3 (1 pkt)\n",
        "\n",
        "Wypisz N najczęściej występujących tokenów. Wynik przedstaw na wykresie słupkowym.\n",
        "\n",
        "(wykorzystaj dowolną bibiliotekę Pythona do generowania wykresów)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6PobJUewBJe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def top_N_tokens(N, collection):\n",
        "  \n",
        "\n",
        "    ## wpisz treść swojej funkcji\n",
        "    \n",
        "    return ...\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTooz6uIwLAs"
      },
      "source": [
        "## Ćwiczenie 4 (2 pkt)\n",
        "\n",
        "## **Poprawa literówek**\n",
        "\n",
        "Jeżeli klasyfikujemy teksty na podstawie słownika, nie chcielibyśmy, aby jakieś słowo dziedzinowe nie zostało uwzględnione w zliczeniu wyłącznie z powodu błędnego zapisu, prawda? Na szczęście, możemy automatycznie poprawiać literówki wykorzystując znane funkcje obliczające odległości pomiędzy słowami, oraz korpus poprawnie zapisanych słów z języka angielskiego, który udostępnia nam bibilioteka NLTK.\n",
        "\n",
        "Zapoznaj się z [artykułem na Wikipedii na temat pojęcia odległości Damerau–Levenshteina](https://en.wikipedia.org/wiki/Damerau–Levenshtein_distance) (inaczej - odległości edycyjnej). \n",
        "\n",
        "Odległość edycyjna to jedna z najpopularniejszych funkcji pozwalających na obliczenie podobieństwa słów. \"Odległość\" pomiędzy słowami obliczana jest na podstawie liczby usunięć, podstawień i dodań, które trzeba wykonać, aby jedno słowo zamienić na inne.\n",
        "\n",
        "Wykorzystaj odległość edycyjną do stworzenia funkcji rekomendacyjnej, która będzie poprawiała słowa zawierające literówki."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCdmfr85xSxb"
      },
      "source": [
        "from nltk.corpus import words\n",
        "nltk.download('words')\n",
        "\n",
        "correct_spellings = words.words() # lista poprawnie zapisanych słów ze słownika języka angielskiego z NLTK\n",
        "\n",
        "def spelling_recommender(entries=['cormulent', 'validrate', 'incendence']):\n",
        "    \n",
        "    return ...\n",
        "    \n",
        "spelling_recommender()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model BM25\n",
        "\n",
        "W tej sekcji należy zaimplementować prostą wyszukiwarkę dokumentów na podstawie zapytania. W tym celu należy skorzystać z [biblioteki rank_bm25](https://pypi.org/project/rank-bm25/) i odpowiednio przygotować dane wejściowe."
      ],
      "metadata": {
        "id": "dA5mlzWEhucJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install rank_bm25"
      ],
      "metadata": {
        "id": "CMLroKsHepil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Przykład 3\n",
        "\n",
        "Poniższy przykład przedstawia model wyszukiwarki na surowych danych z korpusu tekstów agencji Reutera."
      ],
      "metadata": {
        "id": "c5Guh3xKiuAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "bm25 = BM25Okapi(tokenized_reuters)\n",
        "\n",
        "query = \"coffee market\"\n",
        "tokenized_query = query.split(\" \")"
      ],
      "metadata": {
        "id": "C14AHM2LYnOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Możemy pozyskać wyniki zwracane przez funkcję podobieństwa w odniesieniu do wszystkich dokumentów w korpusie..."
      ],
      "metadata": {
        "id": "a-EqCrfgiyeD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc_scores = bm25.get_scores(tokenized_query)\n",
        "doc_scores"
      ],
      "metadata": {
        "id": "oWyT_cmYf8jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... lub otrzymać gotowy ranking top N dopasowanych do zapytania tekstów."
      ],
      "metadata": {
        "id": "P5wMKHOTi6nP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bm25.get_top_n(tokenized_query, reuters_paragraphs, n=1)"
      ],
      "metadata": {
        "id": "4rVoLk89guwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ćwiczenie 5 (3 pkt)\n",
        "\n",
        "Napisz skrypt, który zwróci listę list tokenów dla korpusu Reuters. Lista *tokenized_and_preprocessed_reuters* powinna:\n",
        "1. zawierać tokeny zapisane wyłącznie małymi literami\n",
        "2. mieć usunięte tzw. 'stop words' (czyli słowa takie jak “the”, “a”, “an”, “in\" - lista takich słów dostępna jest w pakiecie NLTK)...\n",
        "3.  oraz znaki interpunkcyjne (możesz skorzystać z wyrażeń regularnych)\n",
        "3. zawierać zlematyzowane słowa\n",
        "4. zawierać słowa po korekcie literówek\n",
        "5. mieć usunięte najczęstsze słowa z korpusu (wykorzystaj część funkcji z ćwiczenia 3, sam_a wybierz punkt odcięcia)\n",
        "\n",
        "---\n",
        "\n",
        "Alternatywą lematyzacji jest tzw. stemming. Czym te zadania się różnią? Znajdź odpowiedź w Internecie i streść ją w jednym zdaniu:\n",
        "\n",
        " jest to proces usunięcia ze słowa końcówki fleksyjnej pozostawiający tylko temat wyrazu, czyli część wyrazu nie uczestniczącą w odmianie wyrazu."
      ],
      "metadata": {
        "id": "-bMmhNVMjDk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessed_reuters(tokenized_lemmatized_reuters):\n",
        " \n",
        "  return ..."
      ],
      "metadata": {
        "id": "hI5TwflNjDOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed=preprocessed_reuters(tokenized_p)"
      ],
      "metadata": {
        "id": "yiDnDmlrTfys"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ćwiczenie 6 (2 pkt)\n",
        "\n",
        "Poeksperymentuj ze swoją wyszukiwarką! Czy preprocessing polepszył jej działanie? Sprawdź i porównaj wyniki dla kilku różnych zapytań. Wyniki opisz poniżej:\n",
        "\n",
        "-- *Twoja odpowiedź* --"
      ],
      "metadata": {
        "id": "CRpI8fTLng7r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#not preprocessed:\n",
        "bm25.get_top_n(tokenized_query, reuters_paragraphs, n=1) # zwrócenie nieztokenizowanego teksu, również za pomocą wyszukiwania"
      ],
      "metadata": {
        "id": "nrr0_cz8UZdy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocessed:\n",
        "\n",
        "## wpisz swój kod"
      ],
      "metadata": {
        "id": "NdDlQkc2OXph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tu wpisz swoje wnioski:\n",
        "-\n",
        "-\n",
        "-"
      ],
      "metadata": {
        "id": "44dHEfopWjn2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ZrxdvYg7amj"
      },
      "source": [
        "# 2. Forma i zawartość sprawozdania\n",
        "\n",
        "Uzupełniony projekt w formacie .ipynb należy przesłać na adres ***"
      ]
    }
  ]
}